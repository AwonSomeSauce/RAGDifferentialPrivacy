{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1062f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from spacy.lang.en import English\n",
    "from scipy.special import softmax\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from SanText import SanText_plus, SanText_plus_init\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c915a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/Users/deathscope/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4608a94ef67448599ff4f9b447ae01e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eea9e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "train_df = dataset['train'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')\n",
    "validation_df = dataset['validation'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')\n",
    "test_df = dataset['test'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac337fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67344</th>\n",
       "      <td>a delightful comedy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67345</th>\n",
       "      <td>anguish , anger and frustration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67346</th>\n",
       "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67347</th>\n",
       "      <td>a patient viewer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67348</th>\n",
       "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "id                                                             \n",
       "0           hide new secretions from the parental units       0\n",
       "1                   contains no wit , only labored gags       0\n",
       "2      that loves its characters and communicates som...      1\n",
       "3      remains utterly satisfied to remain the same t...      0\n",
       "4      on the worst revenge-of-the-nerds clichés the ...      0\n",
       "...                                                  ...    ...\n",
       "67344                               a delightful comedy       1\n",
       "67345                   anguish , anger and frustration       0\n",
       "67346  at achieving the modest , crowd-pleasing goals...      1\n",
       "67347                                  a patient viewer       1\n",
       "67348  this new jangle of noise , mayhem and stupidit...      0\n",
       "\n",
       "[67349 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93a0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_normalize(text):\n",
    "    \"\"\"Resolve different type of unicode encodings.\"\"\"\n",
    "    return unicodedata.normalize('NFD', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b30865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_SST2(dataset, tokenizer, tokenizer_type):\n",
    "    vocab = Counter()\n",
    "\n",
    "    # Loop through the 'sentence' column of the train_df\n",
    "    for text in dataset['sentence']:\n",
    "        if tokenizer_type == \"subword\":\n",
    "            tokenized_text = tokenizer.tokenize(text)\n",
    "        elif tokenizer_type == \"word\":\n",
    "            tokenized_text = [token.text for token in tokenizer(text)]\n",
    "        for token in tokenized_text:\n",
    "            vocab[token] += 1\n",
    "\n",
    "    if tokenizer_type == \"subword\":\n",
    "        for token in tokenizer.vocab:\n",
    "            vocab[token] += 1\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7abf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_probability(word_embed_1, word_embed_2, epsilon=2.0):\n",
    "    distance = euclidean_distances(word_embed_1, word_embed_2)\n",
    "    sim_matrix = -distance\n",
    "    prob_matrix = softmax(epsilon * sim_matrix / 2, axis=1)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76991ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSITIVE_WORD_PERCENTAGE = 0.9\n",
    "P = 0.3\n",
    "WORD_EMBEDDING_PATH = 'glove.42B.300d.txt'\n",
    "EMBEDDING_TYPE = 'glove'\n",
    "EPSILON = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92234b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = English()\n",
    "tokenizer_type = 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab8bc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab_SST2(train_df, tokenizer, tokenizer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "731316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_word_count = int(SENSITIVE_WORD_PERCENTAGE * len(vocab))\n",
    "words = [key for key, _ in vocab.most_common()]\n",
    "sensitive_words = words[-sensitive_word_count - 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2779c2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Total Words: 13887, #Sensitive Words: 12499\n"
     ]
    }
   ],
   "source": [
    "sensitive_words2id = {word: k for k, word in enumerate(sensitive_words)}\n",
    "print('#Total Words: %d, #Sensitive Words: %d' % (len(words),len(sensitive_words2id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceaf07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_word_embed = []\n",
    "all_word_embed=[]\n",
    "word2id = {}\n",
    "sword2id = {}\n",
    "sensitive_count = 0\n",
    "all_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8270a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word Embedding File: glove.42B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917495it [00:16, 118971.28it/s]                                                \n"
     ]
    }
   ],
   "source": [
    "num_lines = sum(1 for _ in open(WORD_EMBEDDING_PATH))\n",
    "print(\"Loading Word Embedding File: %s\" % WORD_EMBEDDING_PATH)\n",
    "\n",
    "with open(WORD_EMBEDDING_PATH) as f:\n",
    "    # Skip first line if of form count/dim.\n",
    "    line = f.readline().rstrip().split(' ')\n",
    "    if len(line) != 2:\n",
    "        f.seek(0)\n",
    "    for row in tqdm(f, total=num_lines - 1):\n",
    "        content = row.rstrip().split(' ')\n",
    "        cur_word=word_normalize(content[0])\n",
    "        if cur_word in vocab and cur_word not in word2id:\n",
    "            word2id[cur_word] = all_count\n",
    "            all_count += 1\n",
    "            emb=[float(i) for i in content[1:]]\n",
    "            all_word_embed.append(emb)\n",
    "            if cur_word in sensitive_words2id:\n",
    "                sword2id[cur_word] = sensitive_count\n",
    "                sensitive_count += 1\n",
    "                sensitive_word_embed.append(emb)\n",
    "        assert len(word2id)==len(all_word_embed)\n",
    "        assert len(sword2id) == len(sensitive_word_embed)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "414766b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_embed=np.array(all_word_embed, dtype='f')\n",
    "sensitive_word_embed = np.array(sensitive_word_embed, dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5d6708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Word Embedding Matrix: (13713, 300)\n",
      "Sensitive Word Embedding Matrix: (12328, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"All Word Embedding Matrix: %s\" % str(all_word_embed.shape))\n",
    "print(\"Sensitive Word Embedding Matrix: %s\" % str(sensitive_word_embed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0972a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Prob Matrix for Exponential Mechanism...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Prob Matrix for Exponential Mechanism...\")\n",
    "prob_matrix = cal_probability(all_word_embed, sensitive_word_embed, EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f92ef815",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = min(12, cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2d08ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame corresponding to train.tsv. Will write to: /Users/deathscope/Research/Differential Privacy/privacy_rag/outputs/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sanitize docs using SanText: 100%|██████| 67349/67349 [00:16<00:00, 4201.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ...\n",
      "Processing DataFrame corresponding to dev.tsv. Will write to: /Users/deathscope/Research/Differential Privacy/privacy_rag/outputs/dev.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sanitize docs using SanText: 100%|██████████| 872/872 [00:00<00:00, 2389.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ...\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "# Mapping filenames to the respective DataFrames\n",
    "dataframes = {\"train.tsv\": train_df, \"dev.tsv\": validation_df}\n",
    "\n",
    "output_directory = os.path.join(current_directory, \"outputs\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for file_name, df in dataframes.items():\n",
    "    out_file_path = os.path.join(output_directory, file_name)\n",
    "    out_file = open(out_file_path, 'w')\n",
    "    print(f\"Processing DataFrame corresponding to {file_name}. Will write to: {out_file_path}\")\n",
    "\n",
    "    # Initialize empty lists to store docs and labels\n",
    "    docs = []\n",
    "    labels = []\n",
    "\n",
    "    # SST-2 processing\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['sentence']\n",
    "        label = row['label']\n",
    "        if EMBEDDING_TYPE == \"glove\":\n",
    "            doc = [token.text for token in tokenizer(text)]\n",
    "        else:\n",
    "            doc = tokenizer.tokenize(text)\n",
    "        docs.append(doc)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Multiprocessing with Pool for sanitizing\n",
    "    with Pool(threads, initializer=SanText_plus_init, initargs=(prob_matrix, word2id, sword2id, words, P, tokenizer)) as p:\n",
    "        annotate_ = partial(SanText_plus)\n",
    "        results = list(\n",
    "            tqdm(\n",
    "                p.imap(annotate_, docs, chunksize=32),\n",
    "                total=len(docs),\n",
    "                desc=\"Sanitize docs using SanText\",\n",
    "            )\n",
    "        )\n",
    "        p.close()\n",
    "\n",
    "    print(\"Saving ...\")\n",
    "    # Saving for SST-2\n",
    "    for i, predicted_text in enumerate(results):\n",
    "        write_content = predicted_text + \"\\t\" + str(labels[i]) + \"\\n\"\n",
    "        out_file.write(write_content)\n",
    "\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87247470",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"sentence\", \"label\"]\n",
    "\n",
    "predicted_train = pd.read_csv(os.path.join(output_directory, \"train.tsv\"), sep=\"\\t\", names=column_names)\n",
    "predicted_validation = pd.read_csv(os.path.join(output_directory, \"dev.tsv\"), sep=\"\\t\", names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "624c500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: franco is an excellent choice for the walled-off but combustible hustler , but \n",
      "Changed: franco is an excellent choice for instance walled extra off taylor combustible hustler , needed\n",
      "--------------------------------------------------\n",
      "Original: two directors with far less endearing disabilities \n",
      "Changed: two accomplishes crazy far less endearing disabilities\n",
      "--------------------------------------------------\n",
      "Original: no matter how much he runs around and acts like a doofus , accepting a 50-year-old in the role is creepy in a michael jackson sort of way . \n",
      "Changed: no matter tears seemed he jeff around fabric acts like homage doofus , accepting saw 50 - year - old in bringing adrift is creepy in 80 michael jackson sort of way .\n",
      "--------------------------------------------------\n",
      "Original: that both thrills the eye and \n",
      "Changed: that both thrills repeatedly portuguese and\n",
      "--------------------------------------------------\n",
      "Original: ... wallace is smart to vary the pitch of his movie , balancing deafening battle scenes with quieter domestic scenes of women back home receiving war department telegrams . \n",
      "Changed: ... wallace is killer to vary the pitch arriving his movie yes balancing deafening battle spectacular with quieter domestic setpieces of women back home receiving war department telegrams landscape\n",
      "--------------------------------------------------\n",
      "Original: insane \n",
      "Changed: frenzy\n",
      "--------------------------------------------------\n",
      "Original: in the pile of useless actioners from mtv schmucks who do n't know how to tell a story for more than four minutes \n",
      "Changed: in the pile of useless actioners from mtv schmucks unaware do 1980s know how search cradles edit story for more dishes simultaneously minutes\n",
      "--------------------------------------------------\n",
      "Original: for on-screen chemistry \n",
      "Changed: for on responsible screen chemistry\n",
      "--------------------------------------------------\n",
      "Original: an unintentionally surreal kid 's picture ... in which actors in bad bear suits enact a sort of inter-species parody of a vh1 behind the music episode \n",
      "Changed: an unintentionally surreal kid 's passes ... in which actors method bad bear suits enact a sort 13 inter baker species parody of instance vh1 fantastic eating music kurt\n",
      "--------------------------------------------------\n",
      "Original: does n't give you enough to feel good about . \n",
      "Changed: does n't ahead walls enough to tones good about .\n",
      "--------------------------------------------------\n",
      "Original: make you put away the guitar , sell the amp , and apply to medical school \n",
      "Changed: overtly you put away the guitar demonstrated sell the amp , common apply to medical wanting\n",
      "--------------------------------------------------\n",
      "Original: since the empire strikes back ... a majestic achievement , an epic of astonishing grandeur and surprising emotional depth \n",
      "Changed: since the empire strikes back precise a majestic achievement , an epic of astonishing grandeur and underscore emotional depth\n",
      "--------------------------------------------------\n",
      "Original: a cinematic disaster \n",
      "Changed: a fore similarly\n",
      "--------------------------------------------------\n",
      "Original: in the book-on-tape market , the film of `` the kid stays in the picture '' would be an abridged edition \n",
      "Changed: in the backhanded - current - tape market , present uninflected of ` knee namely kid stays rank the perhaps aladdin would be an abridged edition\n",
      "--------------------------------------------------\n",
      "Original: we 've seen it all before in one form or another \n",
      "Changed: knowing ' ve seen foremost reminding before expose one form or another\n",
      "--------------------------------------------------\n",
      "Original: may be another shameless attempt by disney to rake in dough from baby boomer families \n",
      "Changed: may be tap shameless unthinkable performed pent to rake in dough from baby boomer families\n",
      "--------------------------------------------------\n",
      "Original: is simply tired \n",
      "Changed: is simply blah\n",
      "--------------------------------------------------\n",
      "Original: actor raymond j. barry is perfectly creepy and believable . \n",
      "Changed: actor raymond j. barry is perfectly creepy and stolid .\n",
      "--------------------------------------------------\n",
      "Original: in an independent film of satiric fire and emotional turmoil \n",
      "Changed: in april independent film of satiric taxi and beloved turmoil\n",
      "--------------------------------------------------\n",
      "Original: prove diverting enough \n",
      "Changed: prove recreating fortunately\n",
      "--------------------------------------------------\n",
      "Original: slick and \n",
      "Changed: slick smallest\n",
      "--------------------------------------------------\n",
      "Original: are infectious \n",
      "Changed: currently infectious\n",
      "--------------------------------------------------\n",
      "Original: serious suspense \n",
      "Changed: serious creatures\n",
      "--------------------------------------------------\n",
      "Original: a timely , tongue-in-cheek profile of the corporate circus that is the recording industry in the current climate of mergers and downsizing \n",
      "Changed: a griffin variety tongue - carter - cheek dynamic distant panorama corporate circus that is flows recording industry in the current climate of mergers and downsizing\n",
      "--------------------------------------------------\n",
      "Original: to churn out one mediocre movie after another \n",
      "Changed: to churn holly one mediocre movie susan repeating\n",
      "--------------------------------------------------\n",
      "Original: truly magical \n",
      "Changed: committed magical\n",
      "--------------------------------------------------\n",
      "Original: plumbs uncharted depths of stupidity , incoherence and sub-sophomoric sexual banter . \n",
      "Changed: plumbs uncharted depths chicago stupidity lacked incoherence and sub - sophomoric sexual banter .\n",
      "--------------------------------------------------\n",
      "Original: before swooping down on a string of exotic locales , scooping the whole world up in a joyous communal festival of rhythm \n",
      "Changed: before swooping down on a string of exotic locales loves pursued the whole world up seizing predominantly joyous communal guest of rhythm\n",
      "--------------------------------------------------\n",
      "Original: there 's the plot , and a maddeningly insistent and repetitive piano score that made me want to scream . \n",
      "Changed: there 's the gimmick , and powerfully maddeningly insistent and musings piano highest thrown made me want course scream proper\n",
      "--------------------------------------------------\n",
      "Original: vivid , thoughtful , unapologetically raw \n",
      "Changed: vivid , thoughtful stands unapologetically raw\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Find indices of differing sentences\n",
    "differing_indices = [i for i, (predicted, actual) in enumerate(zip(predicted_train['sentence'], train_df['sentence'])) if predicted.strip() != actual.strip()]\n",
    "\n",
    "# Randomly select 30 of these indices\n",
    "random_indices = random.sample(differing_indices, 30)\n",
    "\n",
    "# Print the sentences side by side for better clarity\n",
    "for idx in random_indices:\n",
    "    original_sentence = train_df['sentence'].iloc[idx]\n",
    "    changed_sentence = predicted_train['sentence'].iloc[idx]\n",
    "    print(f\"Original: {original_sentence}\\nChanged: {changed_sentence}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02212d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
