{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1062f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from spacy.lang.en import English\n",
    "from scipy.special import softmax\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from SanText import SanText_plus, SanText_plus_init\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c915a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/Users/deathscope/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e063e46a4b473da4a713445df604ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eea9e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "train_df = dataset['train'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')\n",
    "validation_df = dataset['validation'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')\n",
    "test_df = dataset['test'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac337fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67344</th>\n",
       "      <td>a delightful comedy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67345</th>\n",
       "      <td>anguish , anger and frustration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67346</th>\n",
       "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67347</th>\n",
       "      <td>a patient viewer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67348</th>\n",
       "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "id                                                             \n",
       "0           hide new secretions from the parental units       0\n",
       "1                   contains no wit , only labored gags       0\n",
       "2      that loves its characters and communicates som...      1\n",
       "3      remains utterly satisfied to remain the same t...      0\n",
       "4      on the worst revenge-of-the-nerds clichés the ...      0\n",
       "...                                                  ...    ...\n",
       "67344                               a delightful comedy       1\n",
       "67345                   anguish , anger and frustration       0\n",
       "67346  at achieving the modest , crowd-pleasing goals...      1\n",
       "67347                                  a patient viewer       1\n",
       "67348  this new jangle of noise , mayhem and stupidit...      0\n",
       "\n",
       "[67349 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93a0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_normalize(text):\n",
    "    \"\"\"Resolve different type of unicode encodings.\"\"\"\n",
    "    return unicodedata.normalize('NFD', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b30865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_SST2(dataset, tokenizer, tokenizer_type):\n",
    "    vocab = Counter()\n",
    "\n",
    "    # Loop through the 'sentence' column of the train_df\n",
    "    for text in dataset['sentence']:\n",
    "        if tokenizer_type == \"subword\":\n",
    "            tokenized_text = tokenizer.tokenize(text)\n",
    "        elif tokenizer_type == \"word\":\n",
    "            tokenized_text = [token.text for token in tokenizer(text)]\n",
    "        for token in tokenized_text:\n",
    "            vocab[token] += 1\n",
    "\n",
    "    if tokenizer_type == \"subword\":\n",
    "        for token in tokenizer.vocab:\n",
    "            vocab[token] += 1\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7abf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_probability(word_embed_1, word_embed_2, epsilon=2.0):\n",
    "    distance = euclidean_distances(word_embed_1, word_embed_2)\n",
    "    sim_matrix = -distance\n",
    "    prob_matrix = softmax(epsilon * sim_matrix / 2, axis=1)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76991ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSITIVE_WORD_PERCENTAGE = 0.9\n",
    "P = 0.3\n",
    "WORD_EMBEDDING_PATH = 'glove.42B.300d.txt'\n",
    "EMBEDDING_TYPE = 'glove'\n",
    "EPSILON = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92234b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = English()\n",
    "tokenizer_type = 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab8bc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab_SST2(train_df, tokenizer, tokenizer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "731316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_word_count = int(SENSITIVE_WORD_PERCENTAGE * len(vocab))\n",
    "words = [key for key, _ in vocab.most_common()]\n",
    "sensitive_words = words[-sensitive_word_count - 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2779c2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Total Words: 13887, #Sensitive Words: 12499\n"
     ]
    }
   ],
   "source": [
    "sensitive_words2id = {word: k for k, word in enumerate(sensitive_words)}\n",
    "print('#Total Words: %d, #Sensitive Words: %d' % (len(words),len(sensitive_words2id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceaf07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_word_embed = []\n",
    "all_word_embed=[]\n",
    "word2id = {}\n",
    "sword2id = {}\n",
    "sensitive_count = 0\n",
    "all_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8270a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word Embedding File: glove.42B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917495it [00:16, 118339.43it/s]                                                \n"
     ]
    }
   ],
   "source": [
    "num_lines = sum(1 for _ in open(WORD_EMBEDDING_PATH))\n",
    "print(\"Loading Word Embedding File: %s\" % WORD_EMBEDDING_PATH)\n",
    "\n",
    "with open(WORD_EMBEDDING_PATH) as f:\n",
    "    # Skip first line if of form count/dim.\n",
    "    line = f.readline().rstrip().split(' ')\n",
    "    if len(line) != 2:\n",
    "        f.seek(0)\n",
    "    for row in tqdm(f, total=num_lines - 1):\n",
    "        content = row.rstrip().split(' ')\n",
    "        cur_word=word_normalize(content[0])\n",
    "        if cur_word in vocab and cur_word not in word2id:\n",
    "            word2id[cur_word] = all_count\n",
    "            all_count += 1\n",
    "            emb=[float(i) for i in content[1:]]\n",
    "            all_word_embed.append(emb)\n",
    "            if cur_word in sensitive_words2id:\n",
    "                sword2id[cur_word] = sensitive_count\n",
    "                sensitive_count += 1\n",
    "                sensitive_word_embed.append(emb)\n",
    "        assert len(word2id)==len(all_word_embed)\n",
    "        assert len(sword2id) == len(sensitive_word_embed)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "414766b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_embed=np.array(all_word_embed, dtype='f')\n",
    "sensitive_word_embed = np.array(sensitive_word_embed, dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5d6708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Word Embedding Matrix: (13713, 300)\n",
      "Sensitive Word Embedding Matrix: (12328, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"All Word Embedding Matrix: %s\" % str(all_word_embed.shape))\n",
    "print(\"Sensitive Word Embedding Matrix: %s\" % str(sensitive_word_embed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0972a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Prob Matrix for Exponential Mechanism...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Prob Matrix for Exponential Mechanism...\")\n",
    "prob_matrix = cal_probability(all_word_embed, sensitive_word_embed, EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f92ef815",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = min(12, cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2d08ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame corresponding to train.tsv. Will write to: /Users/deathscope/Research/Differential Privacy/privacy_rag/outputs/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sanitize docs using SanText: 100%|██████| 67349/67349 [00:17<00:00, 3846.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ...\n",
      "Processing DataFrame corresponding to dev.tsv. Will write to: /Users/deathscope/Research/Differential Privacy/privacy_rag/outputs/dev.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sanitize docs using SanText: 100%|██████████| 872/872 [00:00<00:00, 2211.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ...\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "# Mapping filenames to the respective DataFrames\n",
    "dataframes = {\"train.tsv\": train_df, \"dev.tsv\": validation_df}\n",
    "\n",
    "output_directory = os.path.join(current_directory, \"outputs\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for file_name, df in dataframes.items():\n",
    "    out_file_path = os.path.join(output_directory, file_name)\n",
    "    out_file = open(out_file_path, 'w')\n",
    "    print(f\"Processing DataFrame corresponding to {file_name}. Will write to: {out_file_path}\")\n",
    "\n",
    "    # Initialize empty lists to store docs and labels\n",
    "    docs = []\n",
    "    labels = []\n",
    "\n",
    "    # SST-2 processing\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['sentence']\n",
    "        label = row['label']\n",
    "        if EMBEDDING_TYPE == \"glove\":\n",
    "            doc = [token.text for token in tokenizer(text)]\n",
    "        else:\n",
    "            doc = tokenizer.tokenize(text)\n",
    "        docs.append(doc)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Multiprocessing with Pool for sanitizing\n",
    "    with Pool(threads, initializer=SanText_plus_init, initargs=(prob_matrix, word2id, sword2id, words, P, tokenizer)) as p:\n",
    "        annotate_ = partial(SanText_plus)\n",
    "        results = list(\n",
    "            tqdm(\n",
    "                p.imap(annotate_, docs, chunksize=32),\n",
    "                total=len(docs),\n",
    "                desc=\"Sanitize docs using SanText\",\n",
    "            )\n",
    "        )\n",
    "        p.close()\n",
    "\n",
    "    print(\"Saving ...\")\n",
    "    # Saving for SST-2\n",
    "    for i, predicted_text in enumerate(results):\n",
    "        write_content = predicted_text + \"\\t\" + str(labels[i]) + \"\\n\"\n",
    "        out_file.write(write_content)\n",
    "\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87247470",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"sentence\", \"label\"]\n",
    "\n",
    "sanitized_train = pd.read_csv(os.path.join(output_directory, \"train.tsv\"), sep=\"\\t\", names=column_names)\n",
    "sanitized_validation = pd.read_csv(os.path.join(output_directory, \"dev.tsv\"), sep=\"\\t\", names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70748eb",
   "metadata": {},
   "source": [
    "# Using Vanilla Presidio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bf25a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e25b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = AnalyzerEngine()\n",
    "anonymizer = AnonymizerEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4122f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\", deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n",
    "\n",
    "pronoun_recognizer = PatternRecognizer(supported_entity=\"PRONOUN\", deny_list=[\"he\", \"He\", \"his\", \"His\", \"she\", \"She\", \"hers\", \"Hers\"])\n",
    "\n",
    "analyzer.registry.add_recognizer(titles_recognizer)\n",
    "analyzer.registry.add_recognizer(pronoun_recognizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f33ec",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "624c500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: forgotten the movie \n",
      "Presidio: forgotten the movie \n",
      "SanText: forgotten lunacy movie\n",
      "--------------------------------------------------\n",
      "Original: end up trying to drown yourself in a lake afterwards \n",
      "Presidio: end up trying to drown yourself in a lake afterwards \n",
      "SanText: backed up trying assert drown yourself in a lake staying\n",
      "--------------------------------------------------\n",
      "Original: , action-packed chiller \n",
      "Presidio: , action-packed chiller \n",
      "SanText: , action - packed chiller\n",
      "--------------------------------------------------\n",
      "Original: of a copenhagen neighborhood coping with the befuddling complications life \n",
      "Presidio: of a <ANONYMIZED> neighborhood coping with the befuddling complications life \n",
      "SanText: of a copenhagen neighborhood coping with overwhelm befuddling complications whatever\n",
      "--------------------------------------------------\n",
      "Original: that lifts your spirits \n",
      "Presidio: that lifts your spirits \n",
      "SanText: horns lifts your spirits\n",
      "--------------------------------------------------\n",
      "Original: most fascinating stories \n",
      "Presidio: most fascinating stories \n",
      "SanText: most understandable stories\n",
      "--------------------------------------------------\n",
      "Original: but enjoyable documentary . \n",
      "Presidio: but enjoyable documentary . \n",
      "SanText: but unwary selby whatever\n",
      "--------------------------------------------------\n",
      "Original: to the filmmakers , ivan is a prince of a fellow , but he comes across as shallow and glib though not mean-spirited , and there 's no indication that he 's been responsible for putting together any movies of particular value or merit . \n",
      "Presidio: to the filmmakers , ivan is a prince of a fellow , but <ANONYMIZED> comes across as shallow and glib though not mean-spirited , and there 's no indication that <ANONYMIZED> 's been responsible for putting together any movies of particular value or merit . \n",
      "SanText: to del filmmakers , ivan is a prince of a fellow consists but he reasons across as shallow rise glib though not mean - uninspired , and there 's supply indication that aliens 's been responsible for putting uncommonly any soft of chosen value or merit indeed\n",
      "--------------------------------------------------\n",
      "Original: shows off a lot of stamina and vitality , \n",
      "Presidio: shows off a lot of stamina and vitality , \n",
      "SanText: shows off a predominantly of stamina and vitality condition\n",
      "--------------------------------------------------\n",
      "Original: imagine anyone managing to steal a movie not only from charismatic rising star jake gyllenhaal but also from accomplished oscar winners susan sarandon , dustin hoffman and holly hunter , yet newcomer ellen pompeo pulls off the feat with aplomb \n",
      "Presidio: imagine anyone managing to steal a movie not only from charismatic rising star <ANONYMIZED> but also from accomplished <ANONYMIZED> winners <ANONYMIZED> , <ANONYMIZED> and <ANONYMIZED> , yet newcomer <ANONYMIZED> pulls off the feat with aplomb \n",
      "SanText: argument heroine characterize to steal a movie not only examining charismatic rising star jake gyllenhaal beguiling also from accomplished 1993 winners susan sarandon , dustin hoffman and holly hunter , yet newcomer ellen pompeo pulls off the feat martha aplomb\n",
      "--------------------------------------------------\n",
      "Original: a cleverly crafted but ultimately hollow mockumentary . \n",
      "Presidio: a cleverly crafted but ultimately hollow mockumentary . \n",
      "SanText: super cleverly complications but wanting hollow mockumentary pit\n",
      "--------------------------------------------------\n",
      "Original: it 's the humanizing stuff that will probably sink the film for anyone who does n't think about percentages all day long . \n",
      "Presidio: it 's the humanizing stuff that will probably sink the film for anyone who does n't think about percentages <ANONYMIZED> . \n",
      "SanText: it controversial the humanizing flies characteristically wallace probably sink pack film pan saying who does n't think fathom percentages all directly long .\n",
      "--------------------------------------------------\n",
      "Original: still shines in her quiet blue eyes \n",
      "Presidio: still shines in her quiet blue eyes \n",
      "SanText: still shines in her praised blue sandra\n",
      "--------------------------------------------------\n",
      "Original: a delightful romantic comedy with plenty of bite . \n",
      "Presidio: a delightful romantic comedy with plenty of bite . \n",
      "SanText: j. delightful main comedy with plenty of bite .\n",
      "--------------------------------------------------\n",
      "Original: undeniably worthy and devastating \n",
      "Presidio: undeniably worthy and devastating \n",
      "SanText: undeniably worthy and jolts\n",
      "--------------------------------------------------\n",
      "Original: if the predictability of bland comfort food appeals to you \n",
      "Presidio: if the predictability of bland comfort food appeals to you \n",
      "SanText: if the predictability of bland comfort food appeals to immediate\n",
      "--------------------------------------------------\n",
      "Original: poetic road movie \n",
      "Presidio: poetic road movie \n",
      "SanText: morality road resuscitate\n",
      "--------------------------------------------------\n",
      "Original: coy but exhilarating , with really solid performances by ving rhames and wesley snipes . \n",
      "Presidio: coy but exhilarating , with really solid performances by ving rhames and <ANONYMIZED> . \n",
      "SanText: coy but exhilarating , with versatile acclaimed beaten crudely ving rhames unmentionable wesley snipes .\n",
      "--------------------------------------------------\n",
      "Original: obnoxiously \n",
      "Presidio: obnoxiously \n",
      "SanText: sublimely\n",
      "--------------------------------------------------\n",
      "Original: the transcendent performance \n",
      "Presidio: the transcendent performance \n",
      "SanText: the transcendent capable\n",
      "--------------------------------------------------\n",
      "Original: happily , some things are immune to the folly of changing taste and attitude . \n",
      "Presidio: happily , some things are immune to the folly of changing taste and attitude . \n",
      "SanText: happily waters some things are immune to weeks folly of changing taste and attitude .\n",
      "--------------------------------------------------\n",
      "Original: the usual whoopee-cushion effort \n",
      "Presidio: the usual whoopee-cushion effort \n",
      "SanText: the usual whoopee - cushion effort\n",
      "--------------------------------------------------\n",
      "Original: builds its multi-character story \n",
      "Presidio: builds its multi-character story \n",
      "SanText: lilo swan multi - hughes individuals\n",
      "--------------------------------------------------\n",
      "Original: a half dozen other trouble-in-the-ghetto flicks \n",
      "Presidio: a half dozen other trouble-in-the-ghetto flicks \n",
      "SanText: a half dozen other trouble - in - the grenier ghetto aficionados\n",
      "--------------------------------------------------\n",
      "Original: sweet , genuine \n",
      "Presidio: sweet , genuine \n",
      "SanText: sweet , prepackaged\n",
      "--------------------------------------------------\n",
      "Original: of movie that comes along only occasionally , one so unconventional , gutsy and perfectly \n",
      "Presidio: of movie that comes along only occasionally , one so unconventional , gutsy and perfectly \n",
      "SanText: missing movie ordeal sickening along resume occasionally nelson indeed so unconventional , gutsy send perfectly\n",
      "--------------------------------------------------\n",
      "Original: of people who sadly are at hostile odds with one another through recklessness and retaliation \n",
      "Presidio: of people who sadly are at hostile odds with one another through recklessness and retaliation \n",
      "SanText: of people who adding are at hostile odds with one another impress recklessness and retaliation\n",
      "--------------------------------------------------\n",
      "Original: instantly forgettable snow-and-stuntwork extravaganza \n",
      "Presidio: instantly forgettable snow-and-stuntwork extravaganza \n",
      "SanText: instantly tucked snow - and - stuntwork extravaganza\n",
      "--------------------------------------------------\n",
      "Original: the acting is amateurish , the cinematography is atrocious , \n",
      "Presidio: the acting is amateurish , the cinematography is atrocious , \n",
      "SanText: the serviceable is amateurish raised the cinematography is atrocious completion\n",
      "--------------------------------------------------\n",
      "Original: obligatory cheap \n",
      "Presidio: obligatory cheap \n",
      "SanText: obligatory zhang\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Find indices of differing sentences\n",
    "differing_indices = [i for i, (predicted, actual) in enumerate(zip(sanitized_train['sentence'], train_df['sentence'])) if predicted.strip() != actual.strip()]\n",
    "\n",
    "# Randomly select 30 of these indices\n",
    "random_indices = random.sample(differing_indices, 30)\n",
    "\n",
    "# Print the sentences side by side for better clarity\n",
    "for idx in random_indices:\n",
    "    original_sentence = train_df['sentence'].iloc[idx]\n",
    "    changed_sentence = sanitized_train['sentence'].iloc[idx]\n",
    "    \n",
    "    analyzer_results = analyzer.analyze(text=original_sentence, language='en')\n",
    "    presidio_sentence = anonymizer.anonymize(\n",
    "        text=original_sentence,\n",
    "        analyzer_results=analyzer_results,    \n",
    "        operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"}), \n",
    "                    \"PHONE_NUMBER\": OperatorConfig(\"mask\", {\"type\": \"mask\", \"masking_char\" : \"*\", \"chars_to_mask\" : 12, \"from_end\" : True})}\n",
    "    )\n",
    "    print(f\"Original: {original_sentence}\\nPresidio: {presidio_sentence.text}\\nSanText: {changed_sentence}\\n{'-'*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
