{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1062f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from spacy.lang.en import English\n",
    "from scipy.special import softmax\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from SanText import SanText_plus, SanText_plus_init\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c915a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/Users/deathscope/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb5d678959d4beca8d83c0069113d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6eea9e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "# train_df = dataset['train'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')\n",
    "# validation_df = dataset['validation'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')\n",
    "# test_df = dataset['test'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6ac337fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        \"sentence\": [\n",
    "            \"I loved the movie because of its plot and character development.\",\n",
    "            \"The acting was subpar, and I was really disappointed.\",\n",
    "            \"One of the best movies I've ever watched. Highly recommended!\",\n",
    "            \"The storyline was predictable and lacked depth.\",\n",
    "            \"Stunning visuals and outstanding performances by the lead actors.\",\n",
    "            \"I wouldn't watch it again. The pace was too slow.\",\n",
    "            \"A cinematic masterpiece that's both touching and captivating.\",\n",
    "            \"The soundtrack perfectly complemented the movie's tone.\",\n",
    "            \"While the movie had a strong start, it failed to maintain that momentum.\",\n",
    "            \"A decent one-time watch, but not something to rave about.\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "train_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b93a0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_normalize(text):\n",
    "    \"\"\"Resolve different type of unicode encodings.\"\"\"\n",
    "    return unicodedata.normalize('NFD', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09b30865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_SST2(dataset, tokenizer, tokenizer_type):\n",
    "    vocab = Counter()\n",
    "\n",
    "    # Loop through the 'sentence' column of the train_df\n",
    "    for text in dataset['sentence']:\n",
    "        if tokenizer_type == \"subword\":\n",
    "            tokenized_text = tokenizer.tokenize(text)\n",
    "        elif tokenizer_type == \"word\":\n",
    "            tokenized_text = [token.text for token in tokenizer(text)]\n",
    "        for token in tokenized_text:\n",
    "            vocab[token] += 1\n",
    "\n",
    "    if tokenizer_type == \"subword\":\n",
    "        for token in tokenizer.vocab:\n",
    "            vocab[token] += 1\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f7abf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_probability(word_embed_1, word_embed_2, epsilon=2.0):\n",
    "    distance = euclidean_distances(word_embed_1, word_embed_2)\n",
    "    sim_matrix = -distance\n",
    "    prob_matrix = softmax(epsilon * sim_matrix / 2, axis=1)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "76991ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSITIVE_WORD_PERCENTAGE = 0.9\n",
    "P = 0.3\n",
    "WORD_EMBEDDING_PATH = 'glove.42B.300d.txt'\n",
    "EMBEDDING_TYPE = 'glove'\n",
    "EPSILON = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92234b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = English()\n",
    "tokenizer_type = 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab8bc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab_SST2(train_df, tokenizer, tokenizer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "731316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_word_count = int(SENSITIVE_WORD_PERCENTAGE * len(vocab))\n",
    "words = [key for key, _ in vocab.most_common()]\n",
    "sensitive_words = words[-sensitive_word_count - 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "317e1bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:  ['.', 'the', 'and', 'I', 'The', 'was', 'movie', ',', 'of', 'watch', 'it', 'A', 'that', \"'s\", 'to', 'loved', 'because', 'its', 'plot', 'character', 'development', 'acting', 'subpar', 'really', 'disappointed', 'One', 'best', 'movies', \"'ve\", 'ever', 'watched', 'Highly', 'recommended', '!', 'storyline', 'predictable', 'lacked', 'depth', 'Stunning', 'visuals', 'outstanding', 'performances', 'by', 'lead', 'actors', 'would', \"n't\", 'again', 'pace', 'too', 'slow', 'cinematic', 'masterpiece', 'both', 'touching', 'captivating', 'soundtrack', 'perfectly', 'complemented', 'tone', 'While', 'had', 'a', 'strong', 'start', 'failed', 'maintain', 'momentum', 'decent', 'one', '-', 'time', 'but', 'not', 'something', 'rave', 'about']\n",
      "SENSITIVE WORDS:  [',', 'of', 'watch', 'it', 'A', 'that', \"'s\", 'to', 'loved', 'because', 'its', 'plot', 'character', 'development', 'acting', 'subpar', 'really', 'disappointed', 'One', 'best', 'movies', \"'ve\", 'ever', 'watched', 'Highly', 'recommended', '!', 'storyline', 'predictable', 'lacked', 'depth', 'Stunning', 'visuals', 'outstanding', 'performances', 'by', 'lead', 'actors', 'would', \"n't\", 'again', 'pace', 'too', 'slow', 'cinematic', 'masterpiece', 'both', 'touching', 'captivating', 'soundtrack', 'perfectly', 'complemented', 'tone', 'While', 'had', 'a', 'strong', 'start', 'failed', 'maintain', 'momentum', 'decent', 'one', '-', 'time', 'but', 'not', 'something', 'rave', 'about']\n"
     ]
    }
   ],
   "source": [
    "print(\"WORDS: \", words)\n",
    "print(\"SENSITIVE WORDS: \", sensitive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2779c2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Total Words: 77, #Sensitive Words: 70\n"
     ]
    }
   ],
   "source": [
    "sensitive_words2id = {word: k for k, word in enumerate(sensitive_words)}\n",
    "print('#Total Words: %d, #Sensitive Words: %d' % (len(words),len(sensitive_words2id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ceaf07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_word_embed = []\n",
    "all_word_embed=[]\n",
    "word2id = {}\n",
    "sword2id = {}\n",
    "sensitive_count = 0\n",
    "all_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8270a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word Embedding File: glove.42B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917495it [00:19, 96945.16it/s]                                                 \n"
     ]
    }
   ],
   "source": [
    "num_lines = sum(1 for _ in open(WORD_EMBEDDING_PATH))\n",
    "print(\"Loading Word Embedding File: %s\" % WORD_EMBEDDING_PATH)\n",
    "\n",
    "with open(WORD_EMBEDDING_PATH) as f:\n",
    "    # Skip first line if of form count/dim.\n",
    "    line = f.readline().rstrip().split(' ')\n",
    "    if len(line) != 2:\n",
    "        f.seek(0)\n",
    "    for row in tqdm(f, total=num_lines - 1):\n",
    "        content = row.rstrip().split(' ')\n",
    "        cur_word=word_normalize(content[0])\n",
    "        if cur_word in vocab and cur_word not in word2id:\n",
    "            word2id[cur_word] = all_count\n",
    "            all_count += 1\n",
    "            emb=[float(i) for i in content[1:]]\n",
    "            all_word_embed.append(emb)\n",
    "            if cur_word in sensitive_words2id:\n",
    "                sword2id[cur_word] = sensitive_count\n",
    "                sensitive_count += 1\n",
    "                sensitive_word_embed.append(emb)\n",
    "        assert len(word2id)==len(all_word_embed)\n",
    "        assert len(sword2id) == len(sensitive_word_embed)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "414766b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_embed=np.array(all_word_embed, dtype='f')\n",
    "sensitive_word_embed = np.array(sensitive_word_embed, dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5d5d6708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Word Embedding Matrix: (70, 300)\n",
      "Sensitive Word Embedding Matrix: (65, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"All Word Embedding Matrix: %s\" % str(all_word_embed.shape))\n",
    "print(\"Sensitive Word Embedding Matrix: %s\" % str(sensitive_word_embed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0972a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Prob Matrix for Exponential Mechanism...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Prob Matrix for Exponential Mechanism...\")\n",
    "prob_matrix = cal_probability(all_word_embed, sensitive_word_embed, EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6744270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9864072e-01 1.0672838e-05 1.4418360e-05 ... 2.7827216e-06\n",
      "  3.4367824e-06 1.1529975e-06]\n",
      " [3.5734470e-03 2.3980686e-02 5.6089237e-02 ... 6.6333573e-04\n",
      "  7.5063779e-04 3.0566554e-04]\n",
      " [2.2675011e-02 1.0487794e-02 6.5108766e-03 ... 1.6250034e-03\n",
      "  1.5021058e-03 5.6710804e-04]\n",
      " ...\n",
      " [2.7835506e-06 1.9244369e-06 1.9149497e-06 ... 9.9893826e-01\n",
      "  8.3348277e-05 6.2886234e-06]\n",
      " [3.4378870e-06 2.4326989e-06 2.4328476e-06 ... 8.3350227e-05\n",
      "  9.9896169e-01 2.3809767e-05]\n",
      " [1.1539561e-06 7.6035218e-07 9.1737854e-07 ... 6.2919771e-06\n",
      "  2.3821905e-05 9.9947101e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(prob_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f92ef815",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = min(12, cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f2d08ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame corresponding to train.tsv. Will write to: /Users/deathscope/Research/Differential Privacy/privacy_rag/outputs/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sanitize docs using SanText: 100%|██████████████| 10/10 [00:02<00:00,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "# Mapping filenames to the respective DataFrames\n",
    "dataframes = {\"train.tsv\": train_df}\n",
    "\n",
    "output_directory = os.path.join(current_directory, \"outputs\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for file_name, df in dataframes.items():\n",
    "    out_file_path = os.path.join(output_directory, file_name)\n",
    "    out_file = open(out_file_path, 'w')\n",
    "    print(f\"Processing DataFrame corresponding to {file_name}. Will write to: {out_file_path}\")\n",
    "\n",
    "    # Initialize empty lists to store docs and labels\n",
    "    docs = []\n",
    "    labels = []\n",
    "\n",
    "    # SST-2 processing\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['sentence']\n",
    "        if EMBEDDING_TYPE == \"glove\":\n",
    "            doc = [token.text for token in tokenizer(text)]\n",
    "        else:\n",
    "            doc = tokenizer.tokenize(text)\n",
    "        docs.append(doc)\n",
    "\n",
    "    # Multiprocessing with Pool for sanitizing\n",
    "    with Pool(threads, initializer=SanText_plus_init, initargs=(prob_matrix, word2id, sword2id, words, P, tokenizer)) as p:\n",
    "        annotate_ = partial(SanText_plus)\n",
    "        results = list(\n",
    "            tqdm(\n",
    "                p.imap(annotate_, docs, chunksize=32),\n",
    "                total=len(docs),\n",
    "                desc=\"Sanitize docs using SanText\",\n",
    "            )\n",
    "        )\n",
    "        p.close()\n",
    "\n",
    "    print(\"Saving ...\")\n",
    "    # Saving for SST-2\n",
    "    for i, predicted_text in enumerate(results):\n",
    "        write_content = predicted_text + \"\\n\"\n",
    "        out_file.write(write_content)\n",
    "\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87247470",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"sentence\"]\n",
    "\n",
    "sanitized_train = pd.read_csv(os.path.join(output_directory, \"train.tsv\"), sep=\"\\t\", names=column_names)\n",
    "sanitized_validation = pd.read_csv(os.path.join(output_directory, \"dev.tsv\"), sep=\"\\t\", names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70748eb",
   "metadata": {},
   "source": [
    "# Using Vanilla Presidio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8bf25a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e25b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = AnalyzerEngine()\n",
    "anonymizer = AnonymizerEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4122f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\", deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n",
    "\n",
    "pronoun_recognizer = PatternRecognizer(supported_entity=\"PRONOUN\", deny_list=[\"he\", \"He\", \"his\", \"His\", \"she\", \"She\", \"hers\", \"Hers\"])\n",
    "\n",
    "analyzer.registry.add_recognizer(titles_recognizer)\n",
    "analyzer.registry.add_recognizer(pronoun_recognizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f33ec",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "624c500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The acting was subpar, and I was really disappointed.\n",
      "Presidio: The acting was subpar, and I was really disappointed.\n",
      "SanText: subpar acting a subpar , not ever was really disappointed .\n",
      "--------------------------------------------------\n",
      "Original: While the movie had a strong start, it failed to maintain that momentum.\n",
      "Presidio: While the movie had a strong start, it failed to maintain that momentum.\n",
      "SanText: I both movies had a strong start , it failed to maintain that momentum but\n",
      "--------------------------------------------------\n",
      "Original: The storyline was predictable and lacked depth.\n",
      "Presidio: The storyline was predictable and lacked depth.\n",
      "SanText: development storyline was predictable and lacked depth .\n",
      "--------------------------------------------------\n",
      "Original: The soundtrack perfectly complemented the movie's tone.\n",
      "Presidio: The soundtrack perfectly complemented the movie's tone.\n",
      "SanText: One soundtrack perfectly complemented the movie 's tone not\n",
      "--------------------------------------------------\n",
      "Original: One of the best movies I've ever watched. Highly recommended!\n",
      "Presidio: One of the best movies I've ever watched. Highly recommended!\n",
      "SanText: watched of the best movies Stunning 've ever watched . A recommended !\n",
      "--------------------------------------------------\n",
      "Original: Stunning visuals and outstanding performances by the lead actors.\n",
      "Presidio: Stunning visuals and outstanding performances by the lead actors.\n",
      "SanText: the visuals both outstanding performances by really lead actors .\n",
      "--------------------------------------------------\n",
      "Original: A cinematic masterpiece that's both touching and captivating.\n",
      "Presidio: A cinematic masterpiece that's both touching and captivating.\n",
      "SanText: maintain cinematic masterpiece that 's both touching and captivating .\n",
      "--------------------------------------------------\n",
      "Original: I wouldn't watch it again. The pace was too slow.\n",
      "Presidio: I wouldn't watch it again. The pace was too slow.\n",
      "SanText: too would n't watch it again again subpar pace was too slow but\n",
      "--------------------------------------------------\n",
      "Original: A decent one-time watch, but not something to rave about.\n",
      "Presidio: A decent one-time watch, but not something to rave about.\n",
      "SanText: - decent one - time watch , but not something to rave about really\n",
      "--------------------------------------------------\n",
      "Original: I loved the movie because of its plot and character development.\n",
      "Presidio: I loved the movie because of its plot and character development.\n",
      "SanText: because loved the movie because of its plot had character development because\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Find indices of differing sentences\n",
    "differing_indices = [i for i, (predicted, actual) in enumerate(zip(sanitized_train['sentence'], train_df['sentence'])) if predicted.strip() != actual.strip()]\n",
    "\n",
    "# Randomly select 30 of these indices\n",
    "random_indices = random.sample(differing_indices, 10)\n",
    "\n",
    "# Print the sentences side by side for better clarity\n",
    "for idx in random_indices:\n",
    "    original_sentence = train_df['sentence'].iloc[idx]\n",
    "    changed_sentence = sanitized_train['sentence'].iloc[idx]\n",
    "    \n",
    "    analyzer_results = analyzer.analyze(text=original_sentence, language='en')\n",
    "    presidio_sentence = anonymizer.anonymize(\n",
    "        text=original_sentence,\n",
    "        analyzer_results=analyzer_results,    \n",
    "        operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"<ANONYMIZED>\"}), \n",
    "                    \"PHONE_NUMBER\": OperatorConfig(\"mask\", {\"type\": \"mask\", \"masking_char\" : \"*\", \"chars_to_mask\" : 12, \"from_end\" : True})}\n",
    "    )\n",
    "    print(f\"Original: {original_sentence}\\nPresidio: {presidio_sentence.text}\\nSanText: {changed_sentence}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1700b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
