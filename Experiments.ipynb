{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613413bc",
   "metadata": {},
   "source": [
    "# Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb79f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.spatial.distance as sp_dist\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from spacy.lang.en import English\n",
    "from scipy.special import softmax\n",
    "from mechanisms.detectors.presidio_detector import PresidioDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ecb6d",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7c18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB_FILE_PATH = \"echr_train.json\"\n",
    "WORD_EMBEDDING_PATH = \"glove.840B.300d.txt\"\n",
    "TOP_K = 20\n",
    "EPSILON = 1\n",
    "P = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028b289",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf34e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_distances(distances):\n",
    "    \"\"\"Normalize the given distances\"\"\"\n",
    "    distance_range = max(distances) - min(distances)\n",
    "    # Check if distance_range is zero (i.e., all distances are the same)\n",
    "    if distance_range == 0:\n",
    "        # If so, return an array of zeros (or some other default value)\n",
    "        return [0 for _ in distances]\n",
    "    min_distance = min(distances)\n",
    "    return [-(dist - min_distance) / distance_range for dist in distances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec78adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_header(file):\n",
    "    \"\"\"Check if the embeddings file has a header\"\"\"\n",
    "    return len(file.readline().split()) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e005e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    \"\"\"Read a JSON file and return the data\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12143c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_values(data):\n",
    "    \"\"\"Extract 'text' values from data\"\"\"\n",
    "    return [item[\"text\"] for item in data if \"text\" in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_dataset(df):\n",
    "    \"\"\"Build vocabulary from dataset\"\"\"\n",
    "    tokenizer = English()\n",
    "    vocab = Counter()\n",
    "    for text in df[\"sentence\"]:\n",
    "        tokenized_text = [\n",
    "            token.text\n",
    "            for token in tokenizer(text)\n",
    "            if (token.is_alpha or token.is_digit)\n",
    "        ]\n",
    "        vocab.update(tokenized_text)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_embedding_row(row):\n",
    "    \"\"\"Parse a row in the general_embeddings file\"\"\"\n",
    "    content = row.rstrip().split(\" \")\n",
    "    return content[0], [float(i) for i in content[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeadd6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_embedding(\n",
    "    word,\n",
    "    embedding,\n",
    "    vocab,\n",
    "    word_to_id,\n",
    "    general_word_embeddings,\n",
    "):\n",
    "    \"\"\"Process a single word embedding\"\"\"\n",
    "    if word in vocab and word not in word_to_id and not re.match(r\"^\\d+$\", word):\n",
    "        word_to_id[word] = len(general_word_embeddings)\n",
    "        general_word_embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc7e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_embeddings(vocab):\n",
    "    \"\"\"Process word general_embeddings and return arrays and dictionaries for words in the vocabulary\"\"\"\n",
    "    word_to_id = {}\n",
    "    general_word_embeddings = []\n",
    "    num_lines = sum(1 for _ in open(WORD_EMBEDDING_PATH, encoding=\"utf-8\"))\n",
    "\n",
    "    with open(WORD_EMBEDDING_PATH, encoding=\"utf-8\") as file:\n",
    "        if not has_header(file):\n",
    "            file.seek(0)\n",
    "        num_lines = sum(1 for _ in file)\n",
    "        file.seek(0)\n",
    "\n",
    "        for row in tqdm(file, total=num_lines - 1):\n",
    "            word, embedding = parse_embedding_row(row)\n",
    "            process_word_embedding(\n",
    "                word,\n",
    "                embedding,\n",
    "                vocab,\n",
    "                word_to_id,\n",
    "                general_word_embeddings,\n",
    "            )\n",
    "\n",
    "    return (np.asarray(general_word_embeddings), word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = PresidioDetector()\n",
    "data = read_json_file(TAB_FILE_PATH)\n",
    "text_values = extract_text_values(data)\n",
    "tab_df = pd.DataFrame(text_values, columns=[\"sentence\"])\n",
    "vocab = build_vocab_from_dataset(tab_df)\n",
    "words = [key for key, _ in vocab.most_common()]\n",
    "sensitive_words = detector.detect(vocab)\n",
    "processed_data = process_word_embeddings(vocab)\n",
    "(general_embeddings, word_to_id) = processed_data\n",
    "id_to_word = {v: k for k, v in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d016c",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ccd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(general_embeddings[word_to_id[\"Denmark\"]]) # get GloVe embedding of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = sp_dist.cdist(\n",
    "    general_embeddings[word_to_id[\"Denmark\"]].reshape(1, -1),\n",
    "    general_embeddings[word_to_id[\"Norway\"]].reshape(1, -1),\n",
    "    metric=\"minkowski\",\n",
    "    p=P,\n",
    ")[0]\n",
    "\n",
    "print(distance) # get distance between Denmark and Norway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = sp_dist.cdist(\n",
    "    general_embeddings[word_to_id[\"Denmark\"]].reshape(1, -1),\n",
    "    general_embeddings[word_to_id[\"Macedonia\"]].reshape(1, -1),\n",
    "    metric=\"minkowski\",\n",
    "    p=P,\n",
    ")[0]\n",
    "\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5dccd1",
   "metadata": {},
   "source": [
    "# Calculate sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distance_dict = defaultdict(float)\n",
    "\n",
    "for word in sensitive_words:\n",
    "    if word in word_to_id and word not in max_distance_dict:\n",
    "        similar_indices = sp_dist.cdist(\n",
    "            general_embeddings[word_to_id[word]].reshape(1, -1),\n",
    "            general_embeddings,\n",
    "            metric=\"minkowski\",\n",
    "            p=P,\n",
    "        )[0].argsort()[:TOP_K]\n",
    "        max_distance_index = similar_indices[-1]\n",
    "        max_distance = sp_dist.cdist(\n",
    "            general_embeddings[word_to_id[word]].reshape(1, -1),\n",
    "            general_embeddings[max_distance_index].reshape(1, -1),\n",
    "            metric=\"minkowski\",\n",
    "            p=P,\n",
    "        )\n",
    "        max_distance_dict[word] = max_distance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word, sensitivity = max(max_distance_dict.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9256578",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_word)\n",
    "print(sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc813c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find substitute word for 'Denmark' based on exponential mechanism\n",
    "word = \"Denmark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1adc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = sp_dist.cdist(\n",
    "    general_embeddings[word_to_id[word]].reshape(1, -1),\n",
    "    general_embeddings,\n",
    "    metric=\"minkowski\",\n",
    "    p=P,\n",
    ")[0]\n",
    "sim_matrix = -distances\n",
    "pow_scaled_sim_matrix = np.power(sim_matrix, 3)\n",
    "prob_matrix = softmax(EPSILON * sim_matrix / (2 * sensitivity))[0]\n",
    "substitute_idx = np.random.choice(len(prob_matrix), 1, p=prob_matrix)\n",
    "print(id_to_word[substitute_idx[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439cf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define amplification function\n",
    "def amplify_probabilities(probabilities, amplification_factor, similar_indices):\n",
    "    for i in range(len(probabilities)):\n",
    "        if i in similar_indices:\n",
    "            probabilities[i] *= amplification_factor\n",
    "        else:\n",
    "            probabilities[i] *= (1/amplification_factor)\n",
    "    # Renormalize probabilities\n",
    "    total_probability = sum(probabilities)\n",
    "    probabilities = [p/total_probability for p in probabilities]\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_indices = sp_dist.cdist(\n",
    "    general_embeddings[word_to_id[word]].reshape(1, -1),\n",
    "    general_embeddings,\n",
    "    metric=\"minkowski\",\n",
    "    p=P,\n",
    ")[0].argsort()[:50]\n",
    "\n",
    "prob_matrix = amplify_probabilities(prob_matrix, 2, similar_indices)\n",
    "prob_matrix = amplify_probabilities(prob_matrix, 2, similar_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e87550",
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_idx = np.random.choice(len(prob_matrix), 1, p=prob_matrix)\n",
    "print(id_to_word[substitute_idx[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_matrix[word_to_id[\"Denmark\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_matrix[word_to_id[\"Norway\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_matrix[word_to_id[\"Norwegian\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_distribution_for_word(prob_matrix, word):\n",
    "    # Assuming general_embeddings, word_to_id, and index_to_word are defined, as well as P for Minkowski\n",
    "    similar_indices = sp_dist.cdist(\n",
    "        general_embeddings[word_to_id[word]].reshape(1, -1),\n",
    "        general_embeddings,\n",
    "        metric=\"minkowski\",\n",
    "        p=P,\n",
    "    )[0].argsort()[:100]\n",
    "    similar_words = [id_to_word[idx] for idx in similar_indices]\n",
    "    \n",
    "    # Extract probabilities for the similar words\n",
    "    probabilities = [prob_matrix[word_to_id[sim_word]] for sim_word in similar_words]\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.bar(range(len(similar_words)), probabilities, tick_label=similar_words)\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'Probability Distribution for \"{word}\" Among Its 100 Closest Words')\n",
    "    plt.xticks(rotation=90)  # Rotate labels to avoid overlap\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_distribution_for_word(prob_matrix, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9fe6c",
   "metadata": {},
   "source": [
    "# New Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5ec449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.predefined_recognizers import SpacyRecognizer\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39266321",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = AnalyzerEngine()\n",
    "spacy_recognizer = SpacyRecognizer()\n",
    "analyzer.registry.add_recognizer(spacy_recognizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5384e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_header(file):\n",
    "    \"\"\"Check if the embeddings file has a header\"\"\"\n",
    "    return len(file.readline().split()) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116aa002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locations_and_nrps():\n",
    "    \"\"\"Load word embeddings from a file and categorize them.\"\"\"\n",
    "\n",
    "    # Initializing collections\n",
    "    categories = {\n",
    "        \"LOCATION\": {\"words\": [], \"embeddings\": [], \"index_to_word\": [], \"word_to_index\": {}},\n",
    "        \"NRP\": {\"words\": [], \"embeddings\": [], \"index_to_word\": [], \"word_to_index\": {}}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(WORD_EMBEDDING_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "            if not has_header(file):\n",
    "                file.seek(0)\n",
    "            \n",
    "            for row in tqdm(file):\n",
    "                content = row.rstrip().split(\" \")\n",
    "                word, vector = content[0], list(map(float, content[1:]))\n",
    "                analysis_result = analyzer.analyze(text=word, language=\"en\", entities=[\"LOCATION\", \"NRP\"])\n",
    "                \n",
    "                for result in analysis_result:\n",
    "                    entity_type = result.entity_type\n",
    "                    if entity_type in categories:\n",
    "                        cat_dict = categories[entity_type]\n",
    "                        cat_dict[\"words\"].append(word)\n",
    "                        cat_dict[\"embeddings\"].append(vector)\n",
    "                        cat_dict[\"index_to_word\"].append(word)\n",
    "                        cat_dict[\"word_to_index\"][word] = len(cat_dict[\"index_to_word\"]) - 1\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return_values = []\n",
    "    for category, cat_dict in categories.items():\n",
    "        return_values.extend([\n",
    "            np.asarray(cat_dict[\"words\"]),\n",
    "            np.asarray(cat_dict[\"embeddings\"]),\n",
    "            np.asarray(cat_dict[\"index_to_word\"]),\n",
    "            cat_dict[\"word_to_index\"]\n",
    "        ])\n",
    "\n",
    "    return tuple(return_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b60785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [1:06:33, 549.93it/s]\n"
     ]
    }
   ],
   "source": [
    "locations_and_nrps = get_locations_and_nrps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e087fb2",
   "metadata": {},
   "source": [
    "### Location Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6aaaaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_words = locations_and_nrps[0]\n",
    "location_embeddings = locations_and_nrps[1]\n",
    "location_index_to_word = locations_and_nrps[2]\n",
    "location_word_to_index = locations_and_nrps[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e9cbe",
   "metadata": {},
   "source": [
    "### NRP Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19c20c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrp_words = locations_and_nrps[4]\n",
    "nrp_embeddings = locations_and_nrps[5]\n",
    "nrp_index_to_word = locations_and_nrps[6]\n",
    "nrp_word_to_index = locations_and_nrps[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae0d298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Locations: 82198\n",
      "Total number of NRPs: 34246\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of Locations: {len(location_words)}\")\n",
    "print(f\"Total number of NRPs: {len(nrp_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38fe43",
   "metadata": {},
   "source": [
    "#### Example Usages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff2f9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US' 'York' 'U.S.' ... 'pizookie' 'procrastina' 'wwent']\n",
      "[-6.1300e-01  3.5952e-01  6.1369e-01  2.2584e-01  2.1979e-01 -3.8877e-01\n",
      " -5.6797e-01  1.1757e-01 -4.2763e-02  2.0155e+00 -4.5882e-01  9.4706e-02\n",
      " -9.0584e-02 -3.3069e-01  1.2620e-01 -4.1112e-01 -4.1397e-01  1.0438e+00\n",
      "  2.4548e-01 -2.2299e-01  1.2817e-01  1.9747e-01 -1.8282e-01  3.8635e-01\n",
      " -2.0372e-01 -2.6187e-01 -3.8953e-01 -2.0268e-01  3.4471e-01  3.2418e-01\n",
      " -1.2618e-01  5.7121e-01  3.6365e-01 -2.4217e-01  4.1622e-01 -3.3974e-01\n",
      "  2.5321e-01 -5.0867e-01  7.5067e-01  6.8774e-01  1.8997e-01  1.9956e-02\n",
      "  7.8944e-01 -2.0719e-01  1.7613e-01  3.4474e-01  2.0023e-01 -2.9991e-01\n",
      "  7.2304e-02  2.1757e-02 -1.7221e-01 -5.6177e-03 -4.1844e-01  4.6432e-01\n",
      " -5.7699e-02 -1.3214e-01  6.1830e-02 -4.0080e-01  3.1318e-01 -2.5913e-01\n",
      " -2.7152e-01 -8.5833e-02 -5.9223e-02  3.1800e-01  3.1722e-01  9.1133e-02\n",
      " -1.9776e-01  1.8802e-01 -2.2686e-01  2.4194e-01 -1.7460e-01 -2.5992e-02\n",
      " -2.5309e-01  1.4346e-01  4.0506e-01  9.9204e-02 -3.3167e-01 -4.3345e-01\n",
      "  1.4766e-01  1.7497e-01  4.2795e-01 -4.8345e-01  3.2586e-01  4.0218e-02\n",
      "  4.1955e-01 -4.0483e-01 -1.2344e+00 -7.2422e-01  9.2623e-01 -2.2212e-01\n",
      " -4.1198e-01  1.9465e-01 -5.0043e-02  1.5275e-01 -1.2228e-01 -5.9815e-01\n",
      " -6.8534e-01  1.1386e-01 -3.2577e-01 -5.5275e-01  8.9452e-02  4.2037e-01\n",
      "  1.5100e-02  1.0569e-01  2.1216e-02  5.7556e-01 -1.0310e-01 -6.4031e-01\n",
      " -2.5062e-01  3.2388e-01  3.2695e-02  1.2405e-01 -9.6060e-02  2.8270e-01\n",
      " -4.2339e-02  5.2846e-01  5.9532e-01 -8.2002e-01 -1.6223e-01 -6.0853e-01\n",
      " -3.7271e-01  8.0581e-02 -1.5825e-01 -2.8747e-01  1.0775e-01 -2.8509e-01\n",
      "  1.2008e-01  6.0497e-01 -2.1264e-01  1.7114e-02  2.1624e-01 -1.1064e-01\n",
      " -6.4918e-02 -2.2448e-01 -2.3464e-01 -2.8473e-01 -1.8460e-01 -3.4518e-01\n",
      " -2.5156e-01  1.0937e-01  7.1840e-02 -1.1751e-01  6.4296e-01 -4.5219e-01\n",
      " -2.9984e-01 -6.2885e-02 -3.7121e-02  1.4104e-01 -6.0444e-01  1.9391e-01\n",
      " -1.9363e-01  3.8506e-01  7.1964e-02  3.0931e-02  2.5880e-01 -3.8018e-01\n",
      " -1.7975e-01  1.0352e+00 -6.5811e-02  2.0840e-01  6.3872e-02  3.8065e-01\n",
      " -8.6986e-02 -2.1482e-01 -9.6082e-01 -2.5949e-01 -1.7852e-01 -2.7011e-01\n",
      " -4.0743e-02 -2.7619e-01  1.7381e-02 -4.6759e-01 -2.7394e-01  1.5480e-01\n",
      "  3.3298e-01  4.3824e-01 -1.4699e-01 -1.6488e-01  1.0368e-01  3.4976e-01\n",
      " -1.1005e-01 -1.7628e-01 -1.6825e-01 -5.5019e-04 -3.0415e-01 -1.9341e-01\n",
      "  1.2349e-01 -4.8758e-02 -5.3374e-01  7.3409e-01  2.0607e-01 -5.8478e-01\n",
      "  3.9662e-01 -2.6873e-01 -4.3387e-01 -6.2145e-01 -3.6126e-01  1.5332e-01\n",
      " -1.8680e-01 -2.6997e-01  7.1816e-01  6.9794e-01  4.1762e-01  8.3379e-02\n",
      "  7.5496e-01 -3.8006e-02  2.9380e-02 -1.1265e-01  4.7318e-01  1.3434e-01\n",
      "  5.4251e-01 -6.2468e-01  8.8159e-02  1.0356e-01  7.0328e-01  2.0750e-01\n",
      " -4.3031e-01 -5.5845e-01 -7.2344e-01  1.8773e-01 -4.0061e-01  3.6328e-01\n",
      " -2.2986e-01 -5.7546e-01  1.8761e-01  1.3913e-01  3.6856e-02  2.4418e-01\n",
      "  6.1644e-02  2.6238e-01  5.2010e-01  3.3577e-01  3.5143e-01  1.1228e-01\n",
      "  8.9834e-02  1.2624e-01  2.3629e-02  3.5818e-01  5.3942e-01  1.4937e-01\n",
      "  2.5491e-02 -4.8973e-01 -2.0506e-01  5.4934e-01  2.9777e-01 -7.3545e-02\n",
      " -2.5515e-01  4.1322e-01  2.7757e-01  1.8852e-02  2.2352e-01  1.1387e-01\n",
      "  3.2108e-01  2.2628e-01  2.7173e-02  2.2301e-01  4.6981e-01  2.2032e-01\n",
      " -2.8665e-01  3.9778e-01 -1.6876e-01  3.8379e-01 -2.1579e-01 -1.4430e-01\n",
      " -5.1968e-01 -2.9198e-01 -1.6681e-01  4.1249e-01  3.0089e-01  1.4697e-01\n",
      "  2.3716e-01  1.1481e-01  1.1124e-01  7.6816e-02  1.2921e-01 -3.7156e-01\n",
      " -6.8703e-02 -1.3224e-01 -1.2919e-01 -6.0712e-02 -1.7348e-01 -3.0219e-03\n",
      "  1.5518e-01  3.8380e-03  1.0471e-01  3.0247e-01 -7.3461e-01  1.5291e-01\n",
      "  1.1040e-01  1.9492e-01 -2.7757e-01 -1.5939e-02 -2.0220e-01 -4.2419e-01\n",
      " -4.4382e-01  2.7567e-01 -2.9367e-01 -2.4161e-01 -1.0364e-01  4.3917e-01]\n",
      "US\n",
      "25\n",
      "[-3.7652e-01 -1.2688e-01 -1.6871e-01  6.4612e-02  5.5541e-02 -6.9541e-01\n",
      " -4.7242e-01  5.9741e-01 -5.9618e-01  1.7538e+00 -6.9293e-01 -5.4719e-01\n",
      " -7.7370e-02 -1.8878e-01  4.2074e-01 -1.0491e-01 -4.7112e-02  8.3144e-01\n",
      "  6.0189e-01  8.5523e-01  3.9098e-01  2.4081e-01 -4.9028e-01  1.7930e-01\n",
      " -2.5434e-01  2.3374e-01  3.8507e-01 -4.7687e-01 -3.8558e-01 -2.6680e-01\n",
      "  9.7396e-02 -3.0264e-01 -4.1317e-01 -4.7863e-01  3.2581e-01 -5.5639e-01\n",
      "  1.5388e-01 -6.5447e-02  3.2508e-01  5.2349e-01 -3.5186e-01 -3.9847e-01\n",
      " -7.1400e-02  8.1662e-01  5.5240e-02  1.2936e-01 -4.9775e-01 -3.1632e-01\n",
      " -3.7260e-02 -7.7616e-01 -4.6156e-01  1.0051e-04 -7.1487e-01  2.8376e-01\n",
      " -2.0112e-01 -2.5712e-01  3.0434e-01 -2.2025e-01 -1.3343e-01 -7.6815e-01\n",
      " -5.7778e-01 -2.2549e-01  2.0465e-01 -3.4303e-01 -2.5758e-01 -2.1745e-01\n",
      " -1.5730e-01  4.3997e-01 -3.8542e-01  2.5127e-01 -4.6636e-01  1.0400e-01\n",
      " -4.9812e-01  2.9713e-01  1.7496e-02  1.1264e-01 -4.9155e-01 -4.3552e-01\n",
      " -3.5840e-01  5.7214e-01  1.1636e-01 -1.2714e-01  3.3608e-01  3.5486e-01\n",
      " -1.3438e-01 -1.0630e+00 -1.4718e+00 -5.6913e-02  3.6039e-02  4.2201e-01\n",
      " -2.4032e-01  1.8390e-01 -5.2663e-01  2.8928e-01  2.4640e-02 -2.4621e-02\n",
      "  9.0159e-02 -3.9739e-01  1.2274e-01  5.1818e-01  9.1002e-02  3.3760e-01\n",
      " -3.7093e-01 -5.2723e-02  7.1741e-01  4.5998e-01 -7.8071e-02  9.0480e-02\n",
      "  2.5591e-01  1.4460e-01  4.3777e-01 -5.0942e-01 -5.4106e-01  2.1625e-01\n",
      " -4.5233e-01  6.1380e-01  3.0146e-01 -1.9855e-01 -5.9437e-01 -2.5605e-01\n",
      " -2.7183e-01  6.5241e-01  2.2609e-01 -7.9769e-01  6.4955e-02  1.5894e-01\n",
      "  3.3393e-02  3.7923e-02 -1.7990e-01  1.1160e-01  2.0270e-01  2.1770e-01\n",
      " -1.9846e-01  5.7506e-02 -6.6007e-02 -4.3660e-01 -2.7851e-01 -4.4193e-01\n",
      " -6.9580e-01  3.5801e-02 -5.5623e-01 -9.4665e-01 -8.8483e-02 -6.9381e-01\n",
      " -2.8393e-01  8.7358e-04 -3.3491e-01  3.5444e-01 -3.5618e-02 -1.2331e-01\n",
      " -4.0607e-01  9.0055e-01  7.0632e-01  2.7847e-01 -1.4128e-01 -4.6671e-01\n",
      " -2.2907e-01  2.6327e-01  2.7324e-02  3.7364e-01  4.6810e-01  4.7980e-01\n",
      "  7.6826e-02  3.0341e-02 -1.1254e+00  1.7006e-02  2.3105e-01  2.4783e-01\n",
      " -2.7503e-01 -5.9086e-02 -5.8583e-01 -2.9097e-01 -8.9221e-01  8.6206e-01\n",
      " -3.9278e-01  1.1479e+00 -2.4117e-02  8.2736e-01 -2.0177e-01 -1.9661e-01\n",
      " -2.7424e-01  7.0674e-02 -2.2647e-01 -6.6190e-01 -1.2995e-01 -4.3730e-01\n",
      "  4.4795e-01  5.7117e-02 -8.0875e-02 -1.8677e-01  5.2665e-01  1.9664e-01\n",
      "  2.5095e-01 -5.2883e-01 -3.8187e-01 -8.8193e-01  1.5905e-01 -2.5646e-01\n",
      " -1.3543e-01 -3.4409e-01  6.1218e-01  6.2109e-01  2.3916e-01  2.5390e-01\n",
      "  1.0684e+00 -7.5784e-02  2.8037e-01 -7.2877e-01  6.4560e-01  1.9753e-01\n",
      "  8.6959e-02 -5.6826e-01  3.3232e-01 -7.5883e-01 -1.2446e-01  2.7814e-01\n",
      "  4.6237e-01 -9.4585e-02 -3.4380e-02  4.4314e-01 -5.1697e-01 -6.0678e-02\n",
      "  6.0062e-01 -1.1102e-01 -6.0327e-01 -3.3708e-01  2.6274e-01 -4.8084e-02\n",
      "  7.1449e-02  7.4795e-01  3.2052e-01 -1.6979e-01 -3.0058e-01 -9.4019e-02\n",
      " -2.2131e-01  3.3110e-01 -6.0924e-02 -5.8843e-02  3.2455e-01 -1.9791e-01\n",
      " -5.1900e-01 -2.5243e-02  1.7616e-02  1.9125e-01 -1.0389e-01 -9.8687e-01\n",
      " -3.0609e-01  4.6130e-02 -6.4910e-02 -1.4582e-01  1.0656e-02 -1.4491e-01\n",
      "  3.9942e-02  1.6874e-01 -2.4117e-01  1.2642e-01 -5.3885e-01  3.0517e-01\n",
      "  3.3938e-01  1.6914e-01 -3.0349e-02  2.6250e-01 -7.2358e-01  5.4849e-01\n",
      " -7.4587e-01 -5.5477e-01  2.0912e-02  1.6461e-01  3.8388e-01  5.3733e-02\n",
      "  1.5222e-01  9.8981e-02  2.0840e-01  1.0342e-01  4.2933e-01 -8.9189e-02\n",
      "  3.6742e-01  7.0682e-01 -1.0080e-01  7.6136e-01  2.0719e-01  1.5806e-01\n",
      "  4.7669e-01 -1.1748e-01  6.0151e-01  1.9470e-02 -2.8981e-01  4.7946e-02\n",
      " -4.8566e-01 -1.6350e-01 -3.9444e-01  4.6237e-01  4.4077e-01 -2.4499e-02\n",
      " -4.3003e-01 -5.4413e-01 -2.5838e-01  1.0545e-01  6.5222e-01  1.1604e-01]\n"
     ]
    }
   ],
   "source": [
    "# Print all words classified as locations to verify the list content.\n",
    "print(location_words)\n",
    "\n",
    "# Display the first word's embedding vector from the location_embeddings list for inspection.\n",
    "print(location_embeddings[0])\n",
    "\n",
    "# Reveal the first word in the location_index_to_word list to check the mapping from indices to words.\n",
    "print(location_index_to_word[0])\n",
    "\n",
    "# Retrieve and display the index of 'Japan' in the location_words list.\n",
    "print(location_word_to_index['Japan'])\n",
    "\n",
    "# Extract and print the embedding vector for 'Japan' using its index.\n",
    "print(location_embeddings[location_word_to_index['Japan']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ed330c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(word: str, distance_metric: str) -> list:\n",
    "    \"\"\"\n",
    "    Finds and returns the 20 closest words to the given word based on the specified distance metric.\n",
    "    \n",
    "    Parameters:\n",
    "    - word (str): The word to find similar words for.\n",
    "    - distance_metric (str): The distance metric to use ('cosine', 'euclidean', etc.).\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of the 20 closest words.\n",
    "    \"\"\"\n",
    "    if analyzer.analyze(text=word, language=\"en\", entities=[\"LOCATION\"]):\n",
    "        embeddings = location_embeddings\n",
    "        word_to_index = location_word_to_index\n",
    "        words = location_words\n",
    "    elif word in nrp_word_to_index:\n",
    "        embeddings = nrp_embeddings\n",
    "        word_to_index = nrp_word_to_index\n",
    "        words = nrp_words\n",
    "    else:\n",
    "        print(f\"Word '{word}' not found.\")\n",
    "        return []\n",
    "\n",
    "    # Ensure the word exists in the embeddings\n",
    "    if word not in word_to_index:\n",
    "        print(f\"Word '{word}' does not have an embedding.\")\n",
    "        return []\n",
    "    \n",
    "    word_embedding = embeddings[word_to_index[word]].reshape(1, -1)\n",
    "    distances = sp_dist.cdist(word_embedding, embeddings, metric=distance_metric).flatten()\n",
    "    similar_indices = distances.argsort()[:20]\n",
    "    \n",
    "    return [words[index] for index in similar_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f62a8e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Japan', 'Tokyo', 'Korea', 'Taiwan', 'Osaka', 'China', 'Asia', 'JAPAN', 'japan', 'Kyoto', 'Okinawa', 'Nippon', 'Europe', 'Hokkaido', 'Nagoya', 'Kong', 'Thailand', 'Hong', 'Seoul', 'Kyushu']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "similar_words = get_similar_words('Japan', 'cosine')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f11655",
   "metadata": {},
   "source": [
    "## Pre-compute sensitivities for Intra-Cluster mappings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba1b5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute sensitivity for Location cluster\n",
    "max_distance_dict = defaultdict(float)\n",
    "\n",
    "for word in location_words:\n",
    "    if word not in max_distance_dict:\n",
    "        distances = sp_dist.cdist(\n",
    "            location_embeddings[location_word_to_index[word]].reshape(1, -1),\n",
    "            location_embeddings,\n",
    "            metric='euclidean'\n",
    "        )[0]\n",
    "        max_distance_dict[word] = max(distances)\n",
    "        \n",
    "word_with_greatest_dist = max(\n",
    "    max_distance_dict,\n",
    "    key=max_distance_dict.get\n",
    ")\n",
    "location_sensitivity = max_distance_dict[word_with_greatest_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c71a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute sensitivity for NRP cluster\n",
    "max_distance_dict = defaultdict(float)\n",
    "\n",
    "for word in nrp_words:\n",
    "    if word not in max_distance_dict:\n",
    "        distances = sp_dist.cdist(\n",
    "            nrp_embeddings[nrp_word_to_index[word]].reshape(1, -1),\n",
    "            nrp_embeddings,\n",
    "            metric='euclidean'\n",
    "        )[0]\n",
    "        max_distance_dict[word] = max(distances)\n",
    "        \n",
    "word_with_greatest_dist = max(\n",
    "    max_distance_dict,\n",
    "    key=max_distance_dict.get\n",
    ")\n",
    "nrp_sensitivity = max_distance_dict[word_with_greatest_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0d1bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute inter-cluster sensitivity\n",
    "centroid_embeddings = [\n",
    "    np.mean(location_embeddings, axis=0),  # Centroid for location cluster\n",
    "    np.mean(nrp_embeddings, axis=0)        # Centroid for NRP cluster\n",
    "]\n",
    "cluster_sensitivity = sp_dist.cdist(\n",
    "    centroid_embeddings[0].reshape(1, -1),\n",
    "    centroid_embeddings[1].reshape(1, -1),\n",
    "    metric='euclidean'\n",
    ")[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "676e270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity for Location cluster: 30.714837075298064\n",
      "Sensitivity for NRP cluster: 27.285209268130146\n",
      "Inter-cluster sensitivity: 0.48370653954521364\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sensitivity for Location cluster: {location_sensitivity}\")\n",
    "print(f\"Sensitivity for NRP cluster: {nrp_sensitivity}\")\n",
    "print(f\"Inter-cluster sensitivity: {cluster_sensitivity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee0f53",
   "metadata": {},
   "source": [
    "## Sanitization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dd1aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = \"On a sunny day in Paris, two friends, Alex and Jordan, decided to embark on a leisurely exploration of the city's most charming spots. Alex is Jordanian and Jordan is British. Their journey began at the foot of the iconic Eiffel Tower, where they marveled at the iron lattice structure that towered above them, its peak almost touching the clear blue sky.With a map in hand and a sense of adventure in their hearts, they meandered through the cobblestone streets, making their way to the historic heart of Paris, the Marais. Here, they discovered quaint boutiques, art galleries, and bistros that seemed to have frozen in time. Alex suggested they grab a coffee at a small café tucked away on Rue des Rosiers, a spot he'd heard was beloved by locals and tourists alike for its rich espresso and warm, flaky croissants. As the day unfolded, Jordan, who had a keen interest in art history, insisted they visit the Louvre Museum. They spent hours wandering through the vast halls, admiring masterpieces from different eras and cultures. The highlight was, without a doubt, standing before the Mona Lisa, where they joined a crowd of onlookers, each trying to decipher the enigmatic smile of Leonardo da Vinci's famous subject. The sun was beginning to set, casting a golden hue over the city, when Alex and Jordan found themselves on the banks of the Seine. They decided to cap off their day with a scenic boat cruise, offering them a view of Paris from a different perspective. As the boat glided under the Pont Neuf and past the illuminated Notre-Dame Cathedral, they reflected on the beauty and history that enveloped them at every turn. Their day in Paris was a testament to the enduring charm of the City of Light, a place where every street, every corner, holds a story waiting to be discovered. As they disembarked from the boat, the Eiffel Tower sparkled in the distance, a perfect end to an unforgettable day.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11b186c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a sunny day in Paris, two friends, Alex and Jordan, decided to embark on a leisurely exploration of the city's most charming spots. Alex is Jordanian and Jordan is British. Their journey began at the foot of the iconic Eiffel Tower, where they marveled at the iron lattice structure that towered above them, its peak almost touching the clear blue sky.With a map in hand and a sense of adventure in their hearts, they meandered through the cobblestone streets, making their way to the historic heart of Paris, the Marais. Here, they discovered quaint boutiques, art galleries, and bistros that seemed to have frozen in time. Alex suggested they grab a coffee at a small café tucked away on Rue des Rosiers, a spot he'd heard was beloved by locals and tourists alike for its rich espresso and warm, flaky croissants. As the day unfolded, Jordan, who had a keen interest in art history, insisted they visit the Louvre Museum. They spent hours wandering through the vast halls, admiring masterpieces from different eras and cultures. The highlight was, without a doubt, standing before the Mona Lisa, where they joined a crowd of onlookers, each trying to decipher the enigmatic smile of Leonardo da Vinci's famous subject. The sun was beginning to set, casting a golden hue over the city, when Alex and Jordan found themselves on the banks of the Seine. They decided to cap off their day with a scenic boat cruise, offering them a view of Paris from a different perspective. As the boat glided under the Pont Neuf and past the illuminated Notre-Dame Cathedral, they reflected on the beauty and history that enveloped them at every turn. Their day in Paris was a testament to the enduring charm of the City of Light, a place where every street, every corner, holds a story waiting to be discovered. As they disembarked from the boat, the Eiffel Tower sparkled in the distance, a perfect end to an unforgettable day.\n"
     ]
    }
   ],
   "source": [
    "print(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f85e9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance as sp_dist\n",
    "from scipy.special import softmax\n",
    "from spacy.lang.en import English\n",
    "\n",
    "def sanitize_passage(passage, centroid_embeddings, cluster_sensitivity, location_sensitivity, nrp_sensitivity):\n",
    "    \"\"\"\n",
    "    Sanitizes a passage by replacing specific words with substitutes based on their distance\n",
    "    from the centroids of predefined clusters (locations and NRPs).\n",
    "\n",
    "    Parameters:\n",
    "    - passage (str): The passage to sanitize.\n",
    "    - EPSILON, cluster_sensitivity, location_sensitivity, nrp_sensitivity (float): Parameters controlling the sensitivity of replacements.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of words representing the sanitized passage.\n",
    "    \"\"\"\n",
    "\n",
    "    sanitized_passage = []\n",
    "    tokenizer = English()\n",
    "    tokens = [token.text for token in tokenizer(passage) if token.is_alpha or token.is_digit]\n",
    "\n",
    "    for word in tokens:\n",
    "        # Determine if the word belongs to the location or NRP cluster and set the appropriate embeddings and sensitivity\n",
    "        if word in location_words or word in nrp_words:\n",
    "            word_embedding = location_embeddings[location_word_to_index[word]].reshape(1, -1) if word in location_words else nrp_embeddings[nrp_word_to_index[word]].reshape(1, -1)\n",
    "            sensitivity = location_sensitivity if word in location_words else nrp_sensitivity\n",
    "            # Determine initial cluster choice based on distance to centroids\n",
    "            distances_to_centroids = sp_dist.cdist(word_embedding, np.array(centroid_embeddings), metric='euclidean')[0]\n",
    "            prob_matrix_to_centroids = softmax(EPSILON * (-distances_to_centroids) / (2 * cluster_sensitivity))\n",
    "            initial_cluster_choice = np.random.choice([0, 1], p=prob_matrix_to_centroids)\n",
    "            \n",
    "            # Calculate distances within the chosen cluster\n",
    "            cluster_embeddings = location_embeddings if initial_cluster_choice == 0 else nrp_embeddings\n",
    "            distances_within_cluster = sp_dist.cdist(word_embedding, cluster_embeddings, metric='cosine')[0]\n",
    "            prob_matrix_within_cluster = softmax(EPSILON * (-distances_within_cluster) / (2 * sensitivity))\n",
    "            substitute_idx = np.random.choice(range(len(prob_matrix_within_cluster)), p=prob_matrix_within_cluster)\n",
    "            \n",
    "            chosen_word = location_index_to_word[substitute_idx] if initial_cluster_choice == 0 else nrp_index_to_word[substitute_idx]\n",
    "            sanitized_passage.append(chosen_word)\n",
    "        else:\n",
    "            sanitized_passage.append(word)\n",
    "\n",
    "    return sanitized_passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1e876c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a sunny day in Mesoamericans two friends Alex and Japan-only decided to embark on a leisurely exploration of the city most charming spots Alex is Yasuragi and stockish is Oldan Their journey began at the foot of the iconic Eiffel Tower where they marveled at the iron lattice structure that towered above them its peak almost touching the clear blue sky With a map in hand and a sense of adventure in their hearts they meandered through the cobblestone streets making their way to the historic heart of Theophostic the Marais Here they discovered quaint boutiques art galleries and bistros that seemed to have frozen in time Alex suggested they grab a coffee at a small café tucked away on Rue des Rosiers a spot he heard was beloved by locals and tourists alike for its rich espresso and warm flaky croissants As the day unfolded Houston.The who had a keen interest in art history insisted they visit the Louvre Museum They spent hours wandering through the vast halls admiring masterpieces from different eras and cultures The highlight was without a doubt standing before the Mona Lisa where they joined a crowd of onlookers each trying to decipher the enigmatic smile of Leonardo da Vinci famous subject The sun was beginning to set casting a golden hue over the city when Alex and MiniGolf found themselves on the banks of the Naiya They decided to cap off their day with a scenic boat cruise offering them a view of IRRITANT from a different perspective As the boat glided under the Pont Neuf and past the illuminated Notre Dame Cathedral they reflected on the beauty and history that enveloped them at every turn Their day in Jaworzno was a testament to the enduring charm of the City of Light a place where every street every corner holds a story waiting to be discovered As they disembarked from the boat the Eiffel Tower sparkled in the distance a perfect end to an unforgettable day\n"
     ]
    }
   ],
   "source": [
    "sanitized_passage_text = sanitize_passage(passage, centroid_embeddings, cluster_sensitivity, location_sensitivity, nrp_sensitivity)\n",
    "print(' '.join(sanitized_passage_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147649d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
