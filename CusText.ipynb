{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0797f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3be4fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/Users/deathscope/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390e13ca14544ac690d4ad371a3178a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ba50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "train_df = dataset['train'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')\n",
    "validation_df = dataset['validation'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')\n",
    "test_df = dataset['test'].to_pandas().rename(columns={'idx': 'id'}).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef150c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1.0\n",
    "TOP_K = 20\n",
    "WORD_EMBEDDING_PATH = 'glove.42B.300d.txt'\n",
    "BATCH_SIZE = 64\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e5b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customized_mapping(eps,top_k):\n",
    "    train_corpus = \" \".join(train_df.sentence)\n",
    "    corpus = train_corpus\n",
    "    word_freq = [x[0] for x in Counter(corpus.split()).most_common()]\n",
    "\n",
    "    # Reading from the space-separated file\n",
    "    embeddings = []\n",
    "    idx2word = []\n",
    "    word2idx = {}\n",
    "\n",
    "    with open(WORD_EMBEDDING_PATH, 'r') as f:\n",
    "        # Skip first line if of form count/dim.\n",
    "        line = f.readline().rstrip().split(' ')\n",
    "        if len(line) == 2:\n",
    "            # If the first line is of form count/dim, then we just skip it.\n",
    "            pass\n",
    "        else:\n",
    "            # If the first line isn't count/dim, then it's a valid embedding line. So, we process it.\n",
    "            word = line[0]\n",
    "            vector = list(map(float, line[1:]))\n",
    "            idx2word.append(word)\n",
    "            word2idx[word] = len(idx2word) - 1\n",
    "            embeddings.append(vector)\n",
    "\n",
    "        # Continue processing the rest of the lines\n",
    "        for row in f:\n",
    "            content = row.rstrip().split(' ')\n",
    "            word = content[0]\n",
    "            vector = list(map(float, content[1:]))\n",
    "            idx2word.append(word)\n",
    "            word2idx[word] = len(idx2word) - 1\n",
    "            embeddings.append(vector)\n",
    "\n",
    "    # Converting lists to numpy arrays for consistency with the original code\n",
    "    embeddings = np.asarray(embeddings)\n",
    "    idx2word = np.asarray(idx2word)\n",
    "\n",
    "    word_hash = defaultdict(str)\n",
    "    sim_word_dict = defaultdict(list)\n",
    "    p_dict = defaultdict(list)\n",
    "    for i in trange(len(word_freq)):\n",
    "        word = word_freq[i]\n",
    "        if word in word2idx:\n",
    "            if word not in word_hash:\n",
    "                index_list = euclidean_distances(embeddings[word2idx[word]].reshape(1,-1),embeddings)[0].argsort()[:top_k]\n",
    "                word_list = [idx2word[x] for x in index_list]\n",
    "                embedding_list = np.array([embeddings[x] for x in index_list])    \n",
    "                \n",
    "                for x in word_list:\n",
    "                    if x not in word_hash:\n",
    "                        word_hash[x] = word\n",
    "                        sim_dist_list = euclidean_distances(embeddings[word2idx[x]].reshape(1,-1), embedding_list)[0]\n",
    "                        min_max_dist = max(sim_dist_list) - min(sim_dist_list)\n",
    "                        min_dist = min(sim_dist_list)\n",
    "                        new_sim_dist_list = [-(x-min_dist)/min_max_dist for x in sim_dist_list]\n",
    "                        tmp = [np.exp(eps*x/2) for x in new_sim_dist_list]\n",
    "                        norm = sum(tmp)\n",
    "                        p = [x/norm for x in tmp]\n",
    "                        p_dict[x] = p\n",
    "                        sim_word_dict[x] =  word_list\n",
    "\n",
    "    try:\n",
    "        with open(\"p_dict.txt\", 'w') as json_file:\n",
    "            json_file.write(json.dumps(p_dict, ensure_ascii=False, indent=4))\n",
    "    except IOError:\n",
    "        print(\"Error writing p_dict.txt\")\n",
    "\n",
    "    try:\n",
    "        with open(\"sim_word_dict.txt\", 'w') as json_file:\n",
    "            json_file.write(json.dumps(sim_word_dict, ensure_ascii=False, indent=4))\n",
    "    except IOError:\n",
    "        print(\"Error writing sim_word_dict.txt\")\n",
    "\n",
    "    return sim_word_dict, p_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff38739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 14816/14816 [4:31:30<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "sim_word_dict, p_dict = get_customized_mapping(eps = EPS, top_k = TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dda16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_sents_s1(df,sim_word_dict,p_dict,save_stop_words,type=\"train\"):\n",
    "\n",
    "    punct = list(string.punctuation)\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    cnt = 0 \n",
    "    raw_cnt = 0 \n",
    "    stop_cnt = 0 \n",
    "    dataset = df.sentence\n",
    "    new_dataset = []\n",
    "\n",
    "    for i in trange(len(dataset)):\n",
    "        record = dataset[i].split()\n",
    "        new_record = []\n",
    "        for word in record:\n",
    "            if (save_stop_words and word in stop_words) or (word not in sim_word_dict):\n",
    "                if word in stop_words:\n",
    "                    stop_cnt += 1  \n",
    "                    raw_cnt += 1   \n",
    "                if is_number(word):\n",
    "                    try:\n",
    "                        word = str(round(float(word))+np.random.randint(1000))\n",
    "                    except:\n",
    "                        pass                   \n",
    "                new_record.append(word)\n",
    "            else:\n",
    "                p = p_dict[word]\n",
    "                new_word = np.random.choice(sim_word_dict[word],1,p=p)[0]\n",
    "                new_record.append(new_word)\n",
    "                if new_word == word:\n",
    "                    raw_cnt += 1 \n",
    "\n",
    "            cnt += 1 \n",
    "        new_dataset.append(\" \".join(new_record))\n",
    "\n",
    "    df.sentence = new_dataset\n",
    "\n",
    "    if not os.path.exists(f\"./privatized_dataset/{args.embedding_type}/{args.mapping_strategy}/eps_{args.eps}_top_{args.top_k}_{args.privatization_strategy}_save_stop_words_{args.save_stop_words}\"):\n",
    "        os.mkdir(f\"./privatized_dataset/{args.embedding_type}/{args.mapping_strategy}/eps_{args.eps}_top_{args.top_k}_{args.privatization_strategy}_save_stop_words_{args.save_stop_words}\")\n",
    "    if type == \"train\":\n",
    "        df.to_csv(f\"./privatized_dataset/{args.embedding_type}/{args.mapping_strategy}/eps_{args.eps}_top_{args.top_k}_{args.privatization_strategy}_save_stop_words_{args.save_stop_words}/train.tsv\",\"\\t\",index=0)\n",
    "    else:\n",
    "        df.to_csv(f\"./privatized_dataset/{args.embedding_type}/{args.mapping_strategy}/eps_{args.eps}_top_{args.top_k}_{args.privatization_strategy}_save_stop_words_{args.save_stop_words}/test.tsv\",\"\\t\",index=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2740d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_dataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df=df\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(f\"{MODEL_TYPE}\",do_lower_case=True)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        # get the sentence from the dataframe\n",
    "        sentence = self.df.loc[index,'sentence']\n",
    "\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            sentence,              # sentence to encode\n",
    "            add_special_tokens = True,         # Add '[CLS]' and '[SEP]'\n",
    "            max_length = MAX_LEN,\n",
    "            pad_to_max_length= True,\n",
    "            truncation='longest_first',\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        # These are torch tensors already\n",
    "        input_ids = encoded_dict['input_ids'][0]\n",
    "        attention_mask = encoded_dict['attention_mask'][0]\n",
    "        token_type_ids = encoded_dict['token_type_ids'][0]\n",
    "\n",
    "        #Convert the target to a torch tensor\n",
    "        target = torch.tensor(self.df.loc[index,'label'])\n",
    "\n",
    "        sample = (input_ids,attention_mask,token_type_ids,target)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c315ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:67349,dev_data:872,test_data:1821\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Bert_dataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dataset = Bert_dataset(validation_df)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataset = Bert_dataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"train_data:{len(train_df)},dev_data:{len(validation_df)},test_data:{len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cdcef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
